---
layout: post
mathjax: true
permalink: /exercises/ml/prml/1_1
---
<a href="/exercises/ml/prml/">&#8249; back</a>

**1.1**
<p>
polynomial model: 
\[y(x,\mathbf{w}) = \sum_{j=0}^{M}\mathbf{w}_jx^j\]
j is the order of the polynomial (model): \(j\in[0,M]\)
<br>
loss function is the sum of squared errors: 
$$\mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2$$
N is the number of training examples.
Since the loss is quadratic withrespect to \(\mathbf{w}\), it's derrivative is
linear and has a unique solution, so we can get that by:
\[\nabla \mathcal{L} = 0 \]
We can solve it by breaking it into directional derrivatives of each coefficient:
\frac{\partial\mathcal{L}}{\partialw_i} = \frac{1}{1}\sum_{n=1}^{N}2\frac{\partialy(w_


</p>

